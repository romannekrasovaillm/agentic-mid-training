Если убрать маркетинг, “агентность” у LLM — это способность замкнуть цикл:
наблюдение → (внутреннее состояние/память) → план → действие (tool/среда) → новое наблюдение → корректировка → остановка
Инсайт: mid-training — это место, где у модели формируется “операциональная семантика” действий: не просто “я знаю, что такое API”, а “в этой ситуации действие снижает неопределённость/ошибку”.
Но важный поворот исследований: вместо дорогого RL по среде часто используют “псевдо-RL” через память и языковую рефлексию:
    • Reflexion улучшает агентное поведение через вербальную обратную связь и эпизодическую память (без обновления весов), то есть агентность можно “разогнать” и на уровне контроллера/памяти. (arXiv)
    • Voyager демонстрирует, что даже без fine-tuning можно получить мощную “агентность”, если добавить автоматический curriculum + библиотеку навыков (код) + цикл самопроверки. (arXiv)
Инсайт: RL “весами” — не единственный путь. Часто агентность растёт быстрее через архитектурный цикл обучения/памяти вокруг LLM.

3) Почему “агентность” — это реально фазовое пересечение (синтез твоей схемы)
Твою картину можно усилить так:
Агентность появляется, когда одновременно выполнены 3 условия:
    1. Компетентность: есть что оптимизировать (pretraining + масштаб; эмерджентность). (arXiv)
    2. Операциональные примитивы: модель понимает, что действия меняют наблюдения (mid-training на трассах tool use). (arXiv)
    3. Селектор поведения: модель выбирает долгий цикл вместо “быстрого ответа” (SFT/RLHF/RL, плюс правильная награда по шагам). (arXiv)
И отдельно стоит “4-й множитель”, который многие недооценивают:
    4. Интерфейс и среда (ACIs, инструменты, наблюдения, ограничения): они меняют агентность иногда сильнее, чем +X% в базовой модели. (NeurIPS Proceedings)

4) Что показывают бенчмарки: где агентность ломается
Хорошие интерактивные тесты почти всегда обнаруживают одно и то же:
    • ошибка на шаге 3 “размазывается” и ломает шаги 8–20,
    • модели плохо делают recovery,
    • они чувствительны к качеству наблюдений/интерфейса.
5. Новое измерение Scaling Laws (из Agent-R1)
"As the agent learns to interact more effectively with its environment through multiple tool calls, performance improves proportionally"
Открыто новое измерение масштабирования — частота взаимодействий агента со средой. Больше tool calls → лучше performance.
6. Почему mid-training критичен, но недостаточен
    • Mid-training учит модель знаниям об инструментах 
    • RL post-training учит поведению — когда вызывать tool, как интерпретировать результаты 
7. Data Mixing — ключ к успеху mid-training
Оптимальная пропорция данных:
    • 40-50% — агентные данные (цепочки рассуждений, вызовы инструментов) 
    • 20-30% — сложный код (Python/C++) 
    • Остаток — качественный текст для связности 
    • Роль кода: Обучение на программном коде критично, так как он по своей природе агентен: содержит логику «вход → обработка → выход» и сценарии обработки ошибок.

Резюме фаз развития агентности
Фаза
Роль в формировании агентности
Результат
Pre-training
Накопление «сырых» знаний, логики и понимания кода.
Потенциал: Способность понимать инструкции и мир.
Mid-training
Кристаллизация навыков через траектории рассуждений и работу с инструментами.
Рассуждение: Умение строить маршрут решения и планировать.
Post-training / RL
Активация автономии, обучение через обратную связь и самокоррекцию.
Надежность: Исполнительность и точное следование протоколам.


Мой взгляд
Агентные способности скорее всего эмерджентны на пересечении фаз:
Фаза
Что даёт
Pre-training
Семантическое понимание, world knowledge
Mid-training
Reasoning patterns, tool-use templates
RL
Goal-directed behavior, exploration, feedback loops
Mid-training может быть критичен для потенциала агентности, но RL активирует и усиливает его через reward signal.


Это интересный исследовательский вопрос. Краткий ответ: частично да, но это сложнее.
Что происходит на разных этапах
Pretraining (базовое обучение)
    • Модель получает базовые способности: понимание языка, рассуждение, знания о мире
    • Зачатки агентных паттернов появляются из данных (код, инструкции, диалоги)
    • Но модель ещё не "знает", что она агент
Mid-training / Continued pretraining
    • Часто включает данные с tool use, code execution, multi-step reasoning
    • Здесь модель начинает лучше понимать структуру "думать → действовать → наблюдать"
    • Emergence агентных способностей зависит от состава данных
Post-training (SFT + RLHF/RLAIF)
    • Критический этап для "пробуждения" агентных способностей
    • Модель учится следовать инструкциям, использовать инструменты в нужном формате
    • RLHF помогает с планированием и самокоррекцией
Ключевые наблюдения из исследований
    1. Базовые способности закладываются в pretraining — без них post-training не поможет
    2. Mid-training может усилить специфические навыки (coding, tool use) через targeted data
    3. Формат и послушание приходят в post-training
    4. Scaling laws работают — большие модели показывают emergence раньше
Практический вывод
Агентные способности — это не бинарное "вкл/выкл", а градиент. Mid-training может ускорить их появление, если данные содержат паттерны агентного поведения (traces of tool use, multi-step problem solving, self-correction).


Репозиторий APTBench напрямую исследует этот вопрос — когда и как зарождаются агентные способности в LLM.
Ключевые находки
1. Агентные способности возникают уже в pre-training
APTBench (arXiv 2510.24397) показывает, что базовые модели до fine-tuning уже демонстрируют:
    • Планирование (выбор следующего шага)
    • Использование инструментов (выбор API и параметров)
    • Обработку ошибок
    • Многошаговое рассуждение
2. Mid-training как усилитель
Mid-training (промежуточная фаза между pre-training и RLHF/SFT) — это где агентные способности усиливаются, а не "зарождаются". Типичные данные mid-training:
    • Код и репозитории (→ tool use, debugging)
    • Математические доказательства (→ многошаговое рассуждение)
    • Инструкции и диалоги (→ следование планам)
3. Что говорят исследования
Фаза
Что происходит с агентностью
Pre-training
Эмбриональные способности (паттерны рассуждения, понимание структуры)
Mid-training
Кристаллизация навыков (code reasoning, tool calling patterns)
Post-training
Активация и выравнивание (следование инструкциям, отказ от ошибок)
Вывод
Агентные способности не зарождаются в mid-training — они проявляются и усиливаются там. Семена закладываются в pre-training через:
    • Exposure к коду (if-then паттерны, API вызовы)
    • Математические тексты (цепочки рассуждений)
    • Структурированные данные (JSON, XML → tool schemas)
Mid-training — это скорее "проявитель" латентных способностей, чем их источник.

1. Pretraining (Базовое обучение)
На этом этапе закладывается статистическая вероятность агентного поведения.
    • Механизм: Модель обучается на огромных массивах кода и технической документации. Код по своей природе агентен: он содержит логику «вход → обработка → выход» и обработку ошибок.
    • Результат: В весах модели формируются низкоуровневые представления о причинно-следственных связях. Модель еще не умеет вызывать API, но она уже «видела» миллионы примеров того, как это делается. Без этого фундамента последующее обучение (fine-tuning) приведет лишь к карго-культу: модель будет имитировать формат вызова инструмента, не понимая логики его работы.
2. Mid-training / Continued Pretraining
Здесь происходит доменная адаптация через вливание специализированных данных (Reasoning & Tool-use data).
    • Механизм: В обучающую выборку добавляются «трассы» (traces) — записи работы реальных систем. Например: [Thought] -> [Action] -> [Observation] -> [Final Answer].
    • Emergence (Возникновение): Благодаря Scaling Laws, при достижении определенного объема параметров и качественных данных, модель начинает обобщать навык планирования. Она переходит от предсказания «следующего слова» к предсказанию «следующего логического шага».
    • Результат: Повышается продолжительность контекстуальной памяти и устойчивость логических цепочек (Chain of Thought).
3. Post-training (SFT + RLHF/RLAIF)
Этап выравнивания (alignment) и фиксации интерфейса.
    • SFT (Supervised Fine-Tuning): Модели жестко задают формат вызова инструментов (например, только валидный JSON). Это обучение «синтаксису агентности».
    • RLHF (Reinforcement Learning from Human Feedback): Самый важный этап для автономии.
        ◦ Модель поощряется за самокоррекцию: если инструмент вернул ошибку, и модель в следующей итерации её исправила, она получает высокий reward.
        ◦ Модель учится отказываться от действий, если данных недостаточно, вместо того чтобы галлюцинировать.
    • Результат: Модель становится «управляемой» (steerable) и начинает следовать системному промпту, удерживая роль агента на протяжении сотен токенов.

Техническое резюме: почему это градиент?
Агентность — это способность модели поддерживать длинные причинно-следственные связи в пространстве скрытых состояний.
    1. Pretraining дает «Capacity» (объем знаний и базовую логику).
    2. Mid-training дает «Reasoning» (способность строить маршрут решения).
    3. Post-training дает «Reliability» (надежность выполнения и соблюдение протокола).
Если выкинуть Mid-training, модель будет часто «терять нить» в сложных задачах. Если выкинуть Post-training, модель будет обладать интеллектом, но не сможет взаимодействовать с внешним миром через стандартизированные интерфейсы (как MCP или Tool Call).
