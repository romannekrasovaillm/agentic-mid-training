version: "3.8"

# =============================================================================
# Entropy-Reward Experiments â€” GigaChat3-10B-A1.8B (MoE, 10B total / 1.8B active)
#
# Model: ai-sage/GigaChat3-10B-A1.8B
# VRAM requirement: ~12 GB (bf16, 1.8B active params)
# License: MIT
# =============================================================================

x-common: &common
  build: .
  volumes:
    - ./outputs:/app/outputs
    - ./logs:/app/logs
    - hf-cache:/root/.cache/huggingface
  environment:
    - WANDB_API_KEY=${WANDB_API_KEY:-}
    - HF_TOKEN=${HF_TOKEN:-}
    - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]

services:
  # --- Individual experiments (GRPO training with GigaChat3) ---
  base:
    <<: *common
    container_name: exp-base
    command: ["--config", "configs/base.yaml"]

  adaptive-entropy:
    <<: *common
    container_name: exp-adaptive-entropy
    command: ["--config", "configs/adaptive_entropy.yaml", "--base-config", "configs/base.yaml"]

  partial-format:
    <<: *common
    container_name: exp-partial-format
    command: ["--config", "configs/partial_format.yaml", "--base-config", "configs/base.yaml"]

  loo-baseline:
    <<: *common
    container_name: exp-loo-baseline
    command: ["--config", "configs/loo_baseline.yaml", "--base-config", "configs/base.yaml"]

  jackknife-baseline:
    <<: *common
    container_name: exp-jackknife-baseline
    command: ["--config", "configs/jackknife_baseline.yaml", "--base-config", "configs/base.yaml"]

  full-recipe:
    <<: *common
    container_name: exp-full-recipe
    command: ["--config", "configs/full_recipe.yaml"]

  # --- vLLM inference server (for fast generation with MTP speculative decoding) ---
  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-gigachat3
    ports:
      - "8000:8000"
    volumes:
      - hf-cache:/root/.cache/huggingface
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
      - VLLM_USE_DEEP_GEMM=0
    command: >
      --model ai-sage/GigaChat3-10B-A1.8B
      --dtype auto
      --trust-remote-code
      --max-model-len 4096
      --speculative-config '{"method": "mtp", "num_speculative_tokens": 1}'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # --- Utilities ---
  test:
    <<: *common
    container_name: exp-tests
    entrypoint: ["python", "-m", "pytest"]
    command: ["tests/", "-v", "--tb=short"]
    deploy:
      resources:
        reservations:
          devices: []

  analyze:
    <<: *common
    container_name: exp-analyze
    entrypoint: ["python", "scripts/analyze_results.py"]
    command: ["--output-dir", "outputs"]
    deploy:
      resources:
        reservations:
          devices: []

volumes:
  hf-cache:
