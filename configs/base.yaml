name: "base"
description: "Base configuration â€” constant entropy bonus, strict format reward"

training:
  model_name: "meta-llama/Llama-3.1-8B-Instruct"
  learning_rate: 1.0e-5
  batch_size: 8
  group_size: 4
  max_steps: 5000
  max_seq_len: 2048
  gradient_accumulation_steps: 4
  warmup_ratio: 0.05
  seed: 42
  bf16: true
  output_dir: "outputs/base"
  wandb_project: "entropy-reward-experiments"
  wandb_run_name: "base-const-strict"

entropy:
  strategy: "constant"
  constant_bonus: 0.01

kl:
  enabled: true
  initial_coeff: 0.2
  final_coeff: 0.02
  schedule: "linear"
  warmup_steps: 100
  total_steps: 5000

reward:
  format_mode: "strict"
  format_weight: 1.0
  tool_weight: 1.0
  accuracy_weight: 1.0
  baseline: "group_norm"
  separate_baselines: false

metrics:
  log_interval: 10
  eval_interval: 100
  self_bleu_n: 4
  self_bleu_sample_size: 100
  track_advantage_stats: true
  track_cps: true

eval:
  ood_enabled: true
  ood_datasets: ["format_variation", "tool_variation"]
  metamorphic_enabled: true
  redteam_enabled: true
  redteam_exploit_budget: 50

stop:
  entropy_collapse_threshold: 0.3
  entropy_collapse_window: 50
  diversity_collapse_threshold: 0.4
  hacking_passrate_threshold: 0.8
  hacking_eval_interval: 200
  advantage_drift_threshold: 2.0
  advantage_drift_window: 100
